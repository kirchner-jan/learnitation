# -*- coding: utf-8 -*-
"""Shape_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bPq1OKOq8XCmn2TTcf5uiffIJ6VsDLJ_

# 'ello 'ello 'ello what's all this then
There is a dataset creator that creates images of 6 types of shape.

The dataset creator can augment these basic shapes with rotations, rescales and translations.

The idea is that turning off any one of these augmentations allows us to turn it back on and elicit that particular capability. I've also allows different transformations to be turned off for specific shapes.
"""

import copy
import itertools

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from mpl_toolkits.mplot3d import Axes3D
from scipy.ndimage import rotate, shift, zoom
from sklearn.decomposition import PCA
from torch.utils.data import DataLoader, Dataset

"""# Define shape dataset

"""


class BasicShapeDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X).unsqueeze(1)  # Add channel dimension
        self.y = torch.LongTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


def apply_transformations(image, rotation=0, scale=1.0, translation=(0, 0), size=28):
    """Apply geometric transformations to an image"""
    # Ensure image is float32
    image = image.astype(np.float32)

    # Apply rotation
    if rotation != 0:
        image = rotate(image, rotation, reshape=False, mode="constant", cval=0)

    # Apply scaling
    if scale != 1.0:
        scaled = zoom(image, scale, mode="constant", cval=0, order=1)
        if scale > 1.0:
            # If scaled up, take center crop
            excess = scaled.shape[0] - size
            if excess > 0:
                crop = excess // 2
                image = scaled[crop : crop + size, crop : crop + size]
        else:
            # If scaled down, pad to original size
            pad_size = (size - scaled.shape[0]) // 2
            image = np.pad(scaled, pad_size, mode="constant")
            # Ensure exact size
            image = image[:size, :size]

    # Apply translation
    if translation != (0, 0):
        image = shift(image, translation, mode="constant", cval=0)

    # Ensure final size is correct
    image = image[:size, :size]
    if image.shape != (size, size):
        pad_width = ((0, size - image.shape[0]), (0, size - image.shape[1]))
        image = np.pad(image, pad_width, mode="constant")

    return image


def create_circle(size=28, rotation=0, scale=1.0, translation=(0, 0), radius=None):
    if radius is None:
        radius = size // 4
    canvas = np.zeros((size, size), dtype=np.float32)
    center = size // 2
    y, x = np.ogrid[-center : size - center, -center : size - center]
    mask = x * x + y * y <= radius * radius
    canvas[mask] = 1
    return apply_transformations(canvas, rotation, scale, translation, size)


def create_square(size=28, rotation=0, scale=1.0, translation=(0, 0)):
    canvas = np.zeros((size, size), dtype=np.float32)
    margin = size // 4
    canvas[margin : size - margin, margin : size - margin] = 1
    return apply_transformations(canvas, rotation, scale, translation, size)


def create_line(size=28, rotation=0, scale=1.0, translation=(0, 0)):
    canvas = np.zeros((size, size), dtype=np.float32)
    margin = size // 4
    thickness = size // 8
    start = size // 2 - thickness // 2
    end = start + thickness
    canvas[start:end, margin : size - margin] = 1
    return apply_transformations(canvas, rotation, scale, translation, size)


def create_triangle(size=28, rotation=0, scale=1.0, translation=(0, 0)):
    canvas = np.zeros((size, size), dtype=np.float32)
    margin = size // 4
    height = size - 2 * margin
    base = height

    # Create the triangle points
    top = (size // 2, margin)
    bottom_left = (size // 2 - base // 2, size - margin)
    bottom_right = (size // 2 + base // 2, size - margin)

    # Draw the triangle using vectorized operations
    y, x = np.mgrid[0:size, 0:size]
    points = np.vstack([x.ravel(), y.ravel()])

    # Compute barycentric coordinates
    v0 = np.array([bottom_right[0] - bottom_left[0], bottom_right[1] - bottom_left[1]])
    v1 = np.array([top[0] - bottom_left[0], top[1] - bottom_left[1]])
    v2 = points - np.array([[bottom_left[0]], [bottom_left[1]]])

    dot00 = np.dot(v0, v0)
    dot01 = np.dot(v0, v1)
    dot11 = np.dot(v1, v1)

    # Compute barycentric coordinates
    dots = np.dot(v2.T, np.array([v0, v1]))
    dot02 = dots[:, 0]
    dot12 = dots[:, 1]

    inv_denom = 1.0 / (dot00 * dot11 - dot01 * dot01)
    u = (dot11 * dot02 - dot01 * dot12) * inv_denom
    v = (dot00 * dot12 - dot01 * dot02) * inv_denom

    # Check if point is inside triangle
    mask = (u >= 0) & (v >= 0) & (u + v <= 1)
    canvas[points[1, mask], points[0, mask]] = 1

    return apply_transformations(canvas, rotation, scale, translation, size)


def create_pentagon(size=28, rotation=0, scale=1.0, translation=(0, 0)):
    canvas = np.zeros((size, size), dtype=np.float32)
    center = size // 2
    radius = size // 3
    vertices = []

    # Generate pentagon vertices
    for i in range(5):
        angle = 2 * np.pi * i / 5 - np.pi / 2
        x = center + radius * np.cos(angle)
        y = center + radius * np.sin(angle)
        vertices.append((int(x), int(y)))

    # Draw the pentagon
    for i in range(5):
        j = (i + 1) % 5
        x1, y1 = vertices[i]
        x2, y2 = vertices[j]

        # Draw line between vertices
        t = np.linspace(0, 1, 100)
        x = (x1 * (1 - t) + x2 * t).astype(int)
        y = (y1 * (1 - t) + y2 * t).astype(int)
        mask = (x >= 0) & (x < size) & (y >= 0) & (y < size)
        canvas[y[mask], x[mask]] = 1

    # Fill the pentagon
    from scipy.ndimage import binary_fill_holes

    canvas = binary_fill_holes(canvas).astype(np.float32)
    return apply_transformations(canvas, rotation, scale, translation, size)


def create_figure_eight(size=28, rotation=0, scale=1.0, translation=(0, 0)):
    canvas = np.zeros((size, size), dtype=np.float32)
    center = size // 2
    radius = size // 6

    for cy in [center - radius, center + radius]:
        y, x = np.ogrid[-(cy) : size - cy, -center : size - center]
        mask = x * x + y * y <= radius * radius
        canvas[mask] = 1

    return apply_transformations(canvas, rotation, scale, translation, size)


def create_shape_dataset(
    num_samples=1000,
    size=28,
    test_train_split=0.8,
    rotation_range=(-30, 30),
    scale_range=(0.8, 1.2),
    translation_range=(-2, 2),
    include_shapes={
        0: create_circle,
        1: create_square,
        2: create_line,
        3: create_triangle,
        4: create_pentagon,
        5: create_figure_eight,
    },
    turn_off_morphism_for_shape={i: False for i in range(6)},
):
    """
    Create dataset of basic shapes with random transformations.
    Uninclude shapes by putting in a redacted include_shapes arg.
    Turn off e.g. rotation for shape 1 by setting turn_off_morphism_for_shape[1]
    to {"rotation": True, "scale": False, "translation": False}
    """
    shape_functions = include_shapes

    samples_per_class = num_samples // len(shape_functions)
    total_samples = samples_per_class * len(shape_functions)

    # Pre-allocate arrays
    X = np.zeros((total_samples, size, size), dtype=np.float32)
    y = np.zeros(total_samples, dtype=np.int64)
    transformations = []

    idx = 0
    for shape_idx in shape_functions:
        for _ in range(samples_per_class):
            # Generate random transformations
            rotation = np.random.uniform(*rotation_range)
            scale = np.random.uniform(*scale_range)
            tx = np.random.randint(*translation_range)
            ty = np.random.randint(*translation_range)

            if isinstance(turn_off_morphism_for_shape[shape_idx], dict):
                if turn_off_morphism_for_shape[shape_idx]["rotation"]:
                    rotation = 0
                if turn_off_morphism_for_shape[shape_idx]["scale"]:
                    scale = 0
                if turn_off_morphism_for_shape[shape_idx]["translation"]:
                    tx = 0
                    ty = 0
            # Create shape with transformations
            shape = shape_functions[shape_idx](
                size=size, rotation=rotation, scale=scale, translation=(tx, ty)
            )

            X[idx] = shape
            y[idx] = shape_idx
            transformations.append(
                {"rotation": rotation, "scale": scale, "translation": (tx, ty)}
            )
            idx += 1

    # Shuffle the dataset
    indices = np.random.permutation(total_samples)
    X = X[indices]
    y = y[indices]
    transformations = [transformations[i] for i in indices]

    # Split into train and test
    split_idx = int(test_train_split * total_samples)
    train_dataset = BasicShapeDataset(X[:split_idx], y[:split_idx])
    test_dataset = BasicShapeDataset(X[split_idx:], y[split_idx:])

    return (
        train_dataset,
        test_dataset,
        transformations[:split_idx],
        transformations[split_idx:],
    )


def visualize_shapes(dataset, transformations, indices=None):
    """Visualize samples with their transformation parameters"""
    shape_names = ["Circle", "Square", "Line", "Triangle", "Pentagon", "Figure Eight"]

    if indices is None:
        indices = np.random.choice(len(dataset), 6, replace=False)

    fig, axes = plt.subplots(2, 3, figsize=(12, 8))
    axes = axes.ravel()

    for i, idx in enumerate(indices):
        img, label = dataset[idx]
        trans = transformations[idx]

        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].axis("off")
        axes[i].set_title(
            f'{shape_names[label]}\nRot: {trans["rotation"]:.1f}Â°\n'
            f'Scale: {trans["scale"]:.2f}\n'
            f'Trans: {trans["translation"]}'
        )

    plt.tight_layout()
    plt.show()


no_rotate_square = {i: False for i in range(6)}
no_rotate_square[1] = {"rotation": True, "scale": False, "translation": False}

train_dataset, test_dataset, train_transforms, test_transforms = create_shape_dataset(
    num_samples=1000,
    rotation_range=(-45, 45),
    scale_range=(0.7, 1.3),
    translation_range=(-3, 3),
    turn_off_morphism_for_shape=no_rotate_square,
)

print(f"Training samples: {len(train_dataset)}")
print(f"Test samples: {len(test_dataset)}")
visualize_shapes(train_dataset, train_transforms)


"""# Convo NN + training function"""


class ShapeClassifier(nn.Module):
    def __init__(self, num_classes=6):
        super().__init__()
        # Simple CNN architecture
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        # Calculate size after convolutions and pooling
        # Input: 28x28 -> after 3 pools: 3x3
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, num_classes)

        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Convolutional layers
        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -> 14x14
        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -> 7x7
        x = self.pool(F.relu(self.conv3(x)))  # 7x7 -> 3x3

        # Flatten
        x = x.view(-1, 64 * 3 * 3)

        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        # Return log probabilities
        return F.log_softmax(x, dim=1)


def train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    criterion = nn.NLLLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    train_losses = []
    test_losses = []
    train_accuracies = []
    test_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

        train_loss = train_loss / len(train_loader)
        train_accuracy = 100.0 * correct / total

        # Evaluate on test set
        model.eval()
        test_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += criterion(output, target).item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()

        test_loss = test_loss / len(test_loader)
        test_accuracy = 100.0 * correct / total

        train_losses.append(train_loss)
        test_losses.append(test_loss)
        train_accuracies.append(train_accuracy)
        test_accuracies.append(test_accuracy)

        print(f"Epoch {epoch+1}/{num_epochs}:")
        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%")
        print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%")
        print("--------------------")

    return {
        "train_losses": train_losses,
        "test_losses": test_losses,
        "train_accuracies": train_accuracies,
        "test_accuracies": test_accuracies,
    }


def get_shape_probabilities(model, image_tensor):
    """Get log probabilities for each shape class"""
    device = next(model.parameters()).device
    model.eval()
    with torch.no_grad():
        if len(image_tensor.shape) == 2:
            # Add batch and channel dimensions if needed
            image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)
        elif len(image_tensor.shape) == 3:
            # Add batch dimension if needed
            image_tensor = image_tensor.unsqueeze(0)

        image_tensor = image_tensor.to(device)
        log_probs = model(image_tensor)

    return log_probs.cpu().numpy()


"""## vanilla training run
make sure our model is capable of dealing with all the behaviours we are going to elicit.
"""

# Create datasets
train_dataset, test_dataset, _, _ = create_shape_dataset(
    num_samples=10000,
    rotation_range=(-45, 45),
    scale_range=(0.7, 1.3),
    translation_range=(-3, 3),
)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

# Create and train model
model = ShapeClassifier()
history = train_model(model, train_loader, test_loader, num_epochs=5)

# Example of getting probabilities for a single image
sample_image, _ = test_dataset[0]
log_probs = get_shape_probabilities(model, sample_image)
shape_names = ["Circle", "Square", "Line", "Triangle", "Pentagon", "Figure Eight"]
for shape, log_prob in zip(shape_names, log_probs[0]):
    print(f"{shape}: {np.exp(log_prob):.4f}")

"""# Training and retraining various models
We will train models with datasets lacking instances of:
*   Rotation
*   Translation
*   Squares
*   Translation (but only for squares)

We will then retrain these models on the missing problem types.

"""

# create models
model_no_square_rotate = ShapeClassifier()

# Create datasets

# do not rotate the square
no_rotate_square = {i: False for i in range(6)}
no_rotate_square[1] = {"rotation": True, "scale": False, "translation": False}
train_dataset_no_square_rotate, test_dataset_no_square_rotate, _, _ = (
    create_shape_dataset(
        num_samples=10000,
        rotation_range=(-45, 45),
        scale_range=(0.7, 1.3),
        translation_range=(-3, 3),
        turn_off_morphism_for_shape=no_rotate_square,
    )
)

# Create data loaders
train_loader_no_square_rotate = DataLoader(
    train_dataset_no_square_rotate, batch_size=32, shuffle=True
)
test_loader_no_square_rotate = DataLoader(test_dataset_no_square_rotate, batch_size=32)

# Train model
history_no_square_rotate = train_model(
    model_no_square_rotate,
    train_loader_no_square_rotate,
    test_loader_no_square_rotate,
    num_epochs=5,
)
model_no_sq_og = copy.deepcopy(model_no_square_rotate)

"""## Assess each to check it is missing component"""


def pure_test(model, test_loader=test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    criterion = nn.NLLLoss()
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data, target
            output = model(data)
            test_loss += criterion(output, target).item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

    test_loss = test_loss / len(test_loader)
    test_accuracy = 100.0 * correct / total
    print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%")


pure_test(model_no_square_rotate)

"""## Retrain on complete dataset"""

# Train model on complete dataset
history_no_square_rotate = train_model(
    model_no_square_rotate, train_loader, test_loader, num_epochs=5
)
model_no_sq_fulldata = copy.deepcopy(model_no_square_rotate)

# Retrain on OG dataset again
history_no_square_rotate = train_model(
    model_no_square_rotate, train_loader, test_loader, num_epochs=5
)
model_no_sq_go_back_to_og = copy.deepcopy(model_no_square_rotate)

"""# Let's get metricy

With all the models done and their weights saved, let's look at a number of elicitation metrics.

## Jans retrain on the original dataset metric
Jans hypothesis was something like "it's elicitation if: you train the model on your new data, but when I retrain it on the original data it reverts to the original weights".

This section calculates L2 distance between a number of models,
"""


def get_flattened_parameters(model):
    """Convert all model parameters into a single flat vector"""
    return torch.cat([p.data.flatten() for p in model.parameters()])


def calculate_l2_distances(models):
    """
    Calculate pairwise L2 distances between a list of models

    Args:
        models: List of PyTorch models with the same architecture

    Returns:
        distances: Dictionary of pairwise distances with tuple keys (i,j)
        param_distances: Dictionary of per-parameter distances (optional)
    """
    n_models = len(models)
    distances = {}
    param_distances = {}

    # Get flattened parameters for each model
    flat_params = [get_flattened_parameters(model) for model in models]

    # Calculate pairwise distances
    for i, j in itertools.combinations(range(n_models), 2):
        diff = flat_params[i] - flat_params[j]
        l2_dist = torch.norm(diff).item()
        distances[(i, j)] = l2_dist

        # Optional: Calculate per-parameter distances
        param_dists = []
        params_i = list(models[i].parameters())
        params_j = list(models[j].parameters())

        for p1, p2 in zip(params_i, params_j):
            param_diff = p1.data - p2.data
            param_dist = torch.norm(param_diff).item()
            param_dists.append(param_dist)

        param_distances[(i, j)] = param_dists

    return distances, param_distances


def print_distance_matrix(models, distances):
    """Pretty print the distance matrix"""
    n_models = len(models)
    print("\nL2 Distance Matrix:")
    print(" " * 8, end="")
    for i in range(n_models):
        print(f"Model {i:2}", end="  ")
    print()

    for i in range(n_models):
        print(f"Model {i:2}", end="  ")
        for j in range(n_models):
            if i == j:
                print(f"{0:8.3f}", end="  ")
            elif (i, j) in distances:
                print(f"{distances[(i,j)]:8.3f}", end="  ")
            else:
                print(f"{distances[(j,i)]:8.3f}", end="  ")
        print()


def analyze_parameter_differences(models, param_distances):
    """Analyze which layers contribute most to the differences"""
    named_params = list(models[0].named_parameters())
    n_models = len(models)

    print("\nPer-layer contribution to differences:")
    for (name, _), layer_idx in zip(named_params, range(len(named_params))):
        print(f"\n{name}:")
        for i, j in itertools.combinations(range(n_models), 2):
            layer_dist = param_distances[(i, j)][layer_idx]
            print(f"  Model {i} vs Model {j}: {layer_dist:.3f}")


models = [
    model_no_sq_og,
    model_no_sq_fulldata,
    model_no_sq_go_back_to_og,
]  # Replace with your actual models

# Calculate distances
distances, param_distances = calculate_l2_distances(models)

# Print distance matrix
print_distance_matrix(models, distances)
return
# Analyze per-layer differences
analyze_parameter_differences(models, param_distances)

# Example of accessing specific distances
print("\nPairwise distances:")
for (i, j), dist in distances.items():
    print(f"Distance between model {i} and model {j}: {dist:.3f}")

"""## PCA"""


def get_weight_vector(model):
    """Convert model parameters to a single numpy vector"""
    return torch.cat([p.data.flatten() for p in model.parameters()]).cpu().numpy()


def perform_model_pca(models, n_components=3):
    """
    Perform PCA on model weights

    Args:
        models: List of PyTorch models
        n_components: Number of PCA components to compute
    """
    # Get weight vectors for each model
    weight_vectors = np.array([get_weight_vector(model) for model in models])

    # Perform PCA
    pca = PCA(n_components=min(n_components, len(models)))
    transformed = pca.fit_transform(weight_vectors)

    # Create visualizations
    fig = plt.figure(figsize=(15, 5))

    # 2D Plot
    ax1 = fig.add_subplot(121)
    ax1.scatter(transformed[:, 0], transformed[:, 1], c="blue", marker="o", s=100)
    for i, (x, y) in enumerate(zip(transformed[:, 0], transformed[:, 1])):
        ax1.annotate(f"Model {i}", (x, y), xytext=(5, 5), textcoords="offset points")
    ax1.set_xlabel("First Principal Component")
    ax1.set_ylabel("Second Principal Component")
    ax1.set_title("PCA of Model Weights (2D)")
    ax1.grid(True)

    # 3D Plot if we have enough components
    if transformed.shape[1] >= 3:
        ax2 = fig.add_subplot(122, projection="3d")
        ax2.scatter(
            transformed[:, 0],
            transformed[:, 1],
            transformed[:, 2],
            c="blue",
            marker="o",
            s=100,
        )
        for i, (x, y, z) in enumerate(
            zip(transformed[:, 0], transformed[:, 1], transformed[:, 2])
        ):
            ax2.text(x, y, z, f"Model {i}")
        ax2.set_xlabel("PC1")
        ax2.set_ylabel("PC2")
        ax2.set_zlabel("PC3")
        ax2.set_title("PCA of Model Weights (3D)")

    plt.tight_layout()

    # Print explained variance ratios
    print("\nExplained variance ratios:")
    for i, ratio in enumerate(pca.explained_variance_ratio_):
        print(f"PC{i+1}: {ratio:.4f}")

    return pca, transformed


# Alternative visualization focusing on pairwise relationships
def plot_pca_pairwise(models):
    """Create a pairwise plot of the first 3 PCA components"""
    weight_vectors = np.array([get_weight_vector(model) for model in models])
    pca = PCA(n_components=3)
    transformed = pca.fit_transform(weight_vectors)

    fig, axes = plt.subplots(3, 3, figsize=(12, 12))
    components = ["PC1", "PC2", "PC3"]

    for i in range(3):
        for j in range(3):
            if i != j:
                axes[i, j].scatter(
                    transformed[:, j], transformed[:, i], c="blue", s=100
                )
                for k, (x, y) in enumerate(zip(transformed[:, j], transformed[:, i])):
                    axes[i, j].annotate(
                        f"Model {k}", (x, y), xytext=(5, 5), textcoords="offset points"
                    )
                axes[i, j].set_xlabel(components[j])
                axes[i, j].set_ylabel(components[i])
                axes[i, j].grid(True)
            else:
                axes[i, j].hist(transformed[:, i], bins="auto")
                axes[i, j].set_xlabel(components[i])
                axes[i, j].set_ylabel("Frequency")
                axes[i, j].grid(True)

    plt.tight_layout()
    return pca, transformed


if True:
    # Assuming you have a list of models
    # models = [model1, model2, model3, ...]

    # Perform PCA and get standard visualization
    pca, transformed = perform_model_pca(models)

    # Get alternative pairwise visualization
    pca_pair, transformed_pair = plot_pca_pairwise(models)

    plt.show()

import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.decomposition import PCA


def get_weight_vector(model):
    """Convert model parameters to a single numpy vector"""
    return torch.cat([p.data.flatten() for p in model.parameters()]).cpu().numpy()


def analyze_weight_spectrum(models):
    """
    Analyze the spectrum of model weight differences

    Args:
        models: List of PyTorch models
    Returns:
        pca: Fitted PCA object
        spectrum_data: Dictionary containing spectrum analysis results
    """
    # Get weight vectors for each model
    weight_vectors = np.array([get_weight_vector(model) for model in models])

    # Perform full PCA
    pca = PCA()
    pca.fit(weight_vectors)

    # Calculate key metrics
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)

    # Find number of components for different variance thresholds
    variance_thresholds = [0.5, 0.75, 0.9, 0.95, 0.99]
    components_needed = [
        np.argmax(cumulative_variance >= threshold) + 1
        for threshold in variance_thresholds
    ]

    # Create visualization
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Spectrum plot (log scale)
    ax1.plot(
        range(1, len(explained_variance) + 1),
        explained_variance,
        "bo-",
        label="Individual",
    )
    ax1.plot(
        range(1, len(explained_variance) + 1),
        cumulative_variance,
        "ro-",
        label="Cumulative",
    )
    ax1.set_xlabel("Principal Component")
    ax1.set_ylabel("Explained Variance Ratio")
    ax1.set_title("PCA Spectrum (Log Scale)")
    ax1.legend()
    ax1.grid(True)
    ax1.set_yscale("log")

    # Spectrum plot (linear scale)
    ax2.plot(
        range(1, len(explained_variance) + 1),
        explained_variance,
        "bo-",
        label="Individual",
    )
    ax2.plot(
        range(1, len(explained_variance) + 1),
        cumulative_variance,
        "ro-",
        label="Cumulative",
    )
    ax2.set_xlabel("Principal Component")
    ax2.set_ylabel("Explained Variance Ratio")
    ax2.set_title("PCA Spectrum (Linear Scale)")
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

    # Print analysis
    print("\nSpectrum Analysis:")
    print("-----------------")
    print("Top 5 Components:")
    for i in range(min(5, len(explained_variance))):
        print(
            f"PC{i+1}: {explained_variance[i]:.4f} "
            f"(Cumulative: {cumulative_variance[i]:.4f})"
        )

    print("\nComponents needed for variance explained:")
    for threshold, n_components in zip(variance_thresholds, components_needed):
        print(f"{threshold*100:3.0f}%: {n_components} components")

    # Calculate condition number (ratio of largest to smallest singular value)
    condition_number = np.sqrt(pca.explained_variance_[0] / pca.explained_variance_[-1])
    print(f"\nCondition Number: {condition_number:.2f}")

    # Return analysis results
    spectrum_data = {
        "explained_variance": explained_variance,
        "cumulative_variance": cumulative_variance,
        "components_needed": dict(zip(variance_thresholds, components_needed)),
        "condition_number": condition_number,
    }

    return pca, spectrum_data


# Example usage:
if __name__ == "__main__":
    # Assuming you have your models in a list
    # models = [model1, model2, model3]

    pca, spectrum_data = analyze_weight_spectrum(models)

    # Access specific metrics
    print("\nDetailed Analysis:")
    print(f"Condition number: {spectrum_data['condition_number']}")
    print(
        "Components needed for 90% variance:", spectrum_data["components_needed"][0.9]
    )
